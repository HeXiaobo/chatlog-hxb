"""
æ™ºèƒ½æ–‡ä»¶å¤„ç†å™¨
å®ç°å®Œæ•´çš„èŠå¤©è®°å½• â†’ çŸ¥è¯†åº“æ™ºèƒ½åŒ–è½¬æ¢æµç¨‹
"""
import logging
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict

from app.models.qa import QAPair
from app.models.category import Category
from app.models.upload import UploadHistory
from app import db
from .ai_content_processor import ai_content_processor, KnowledgeBaseEntry
from .ai_classifier import ai_classifier
from .ai_monitor import ai_monitor

logger = logging.getLogger(__name__)


@dataclass
class IntelligentProcessingResult:
    """æ™ºèƒ½å¤„ç†ç»“æœ"""
    success: bool
    upload_id: int
    filename: str
    
    # åŸå§‹æ•°æ®ç»Ÿè®¡
    original_messages: int
    raw_conversations: int
    
    # å†…å®¹åˆ†æç»“æœ
    useful_messages: int
    noise_filtered: int
    content_quality_score: float
    
    # é—®ç­”æå–ç»“æœ
    qa_pairs_extracted: int
    high_quality_pairs: int
    final_knowledge_entries: int
    
    # å¤„ç†ç»Ÿè®¡
    processing_time: float
    ai_processing_time: float
    ai_provider_used: str
    tokens_consumed: int
    processing_cost: float
    
    # è´¨é‡æŒ‡æ ‡
    extraction_efficiency: float  # æœ€ç»ˆæ¡ç›®æ•° / åŸå§‹æ¶ˆæ¯æ•°
    content_improvement_rate: float  # AIæ¸…æ´—åçš„è´¨é‡æå‡
    
    # å¤„ç†æ–¹æ³•
    processing_method: str  # 'ai_intelligent' æˆ– 'rule_based'
    ai_enabled: bool
    
    # è¯¦ç»†ç»Ÿè®¡
    detailed_stats: Dict[str, Any]
    error_message: Optional[str] = None


class IntelligentFileProcessor:
    """æ™ºèƒ½æ–‡ä»¶å¤„ç†å™¨
    
    å®ç°ä½ æè¿°çš„å®Œæ•´æµç¨‹ï¼š
    1. å¯¼å…¥ç­›é€‰çš„èŠå¤©è®°å½•
    2. AIæ™ºèƒ½åˆ†æå’Œå†…å®¹ç­›é€‰
    3. è¿‡æ»¤æ— ç”¨å†…å®¹
    4. æå–æœ‰æ•ˆé—®ç­”å¯¹
    5. æ¸…æ´—å’Œä¼˜åŒ–å†…å®¹
    6. ç”Ÿæˆé«˜è´¨é‡çŸ¥è¯†åº“
    """
    
    def __init__(self):
        self.ai_enabled = True
        
    async def process_file_intelligently(self, 
                                       file_path: Path, 
                                       original_filename: str,
                                       force_ai: bool = False) -> IntelligentProcessingResult:
        """æ™ºèƒ½å¤„ç†èŠå¤©è®°å½•æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            original_filename: åŸå§‹æ–‡ä»¶å
            force_ai: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨AIå¤„ç†
            
        Returns:
            å¤„ç†ç»“æœè¯¦æƒ…
        """
        logger.info(f"å¼€å§‹æ™ºèƒ½å¤„ç†æ–‡ä»¶: {original_filename}")
        start_time = time.time()
        
        # åˆ›å»ºä¸Šä¼ è®°å½•
        upload_record = UploadHistory(
            filename=original_filename,
            file_size=file_path.stat().st_size,
            status='processing'
        )
        db.session.add(upload_record)
        db.session.commit()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šè¯»å–å’Œè§£ææ–‡ä»¶
            logger.info("ğŸ“ æ­¥éª¤1: è¯»å–èŠå¤©è®°å½•æ–‡ä»¶...")
            raw_data = await self._read_chat_file(file_path)
            
            # ç¬¬äºŒæ­¥ï¼šé¢„å¤„ç†å’Œæ•°æ®æ¸…ç†
            logger.info("ğŸ” æ­¥éª¤2: é¢„å¤„ç†èŠå¤©æ•°æ®...")
            processed_messages = self._preprocess_messages(raw_data)
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šå¤„ç†ç­–ç•¥
            use_ai = (self.ai_enabled or force_ai) and self._should_use_ai_processing(processed_messages)
            ai_start_time = time.time()
            
            if use_ai:
                logger.info("ğŸ¤– æ­¥éª¤3: ä½¿ç”¨AIæ™ºèƒ½å¤„ç†...")
                result = await self._ai_intelligent_processing(
                    processed_messages, 
                    original_filename, 
                    upload_record
                )
            else:
                logger.info("ğŸ“‹ æ­¥éª¤3: ä½¿ç”¨è§„åˆ™å¤„ç†...")
                result = await self._rule_based_processing(
                    processed_messages,
                    original_filename,
                    upload_record
                )
                
            ai_processing_time = time.time() - ai_start_time
            
            # ç¬¬å››æ­¥ï¼šä¿å­˜åˆ°æ•°æ®åº“
            logger.info("ğŸ’¾ æ­¥éª¤4: ä¿å­˜çŸ¥è¯†åº“æ¡ç›®...")
            saved_count = await self._save_knowledge_entries(
                result.get('knowledge_entries', []),
                upload_record
            )
            
            # æ›´æ–°ä¸Šä¼ è®°å½•çŠ¶æ€
            total_time = time.time() - start_time
            upload_record.status = 'completed'
            upload_record.total_messages = result.get('statistics', {}).get('original_messages', 0)
            upload_record.extracted_pairs = result.get('statistics', {}).get('final_knowledge_entries', 0)
            upload_record.processing_time = total_time
            db.session.commit()
            
            # æ„å»ºå¤„ç†ç»“æœ
            processing_result = IntelligentProcessingResult(
                success=True,
                upload_id=upload_record.id,
                filename=original_filename,
                
                # åŸå§‹æ•°æ®
                original_messages=result.get('statistics', {}).get('original_messages', 0),
                raw_conversations=len(processed_messages),
                
                # å†…å®¹åˆ†æ
                useful_messages=result.get('statistics', {}).get('useful_messages', 0),
                noise_filtered=result.get('statistics', {}).get('noise_filtered', 0),
                content_quality_score=result.get('statistics', {}).get('content_quality_score', 0),
                
                # é—®ç­”æå–
                qa_pairs_extracted=result.get('statistics', {}).get('qa_pairs_extracted', 0),
                high_quality_pairs=saved_count,
                final_knowledge_entries=saved_count,
                
                # å¤„ç†ç»Ÿè®¡
                processing_time=total_time,
                ai_processing_time=ai_processing_time,
                ai_provider_used=result.get('ai_provider', 'none'),
                tokens_consumed=result.get('tokens_used', 0),
                processing_cost=result.get('processing_cost', 0.0),
                
                # è´¨é‡æŒ‡æ ‡
                extraction_efficiency=result.get('statistics', {}).get('processing_efficiency', 0),
                content_improvement_rate=0.25 if use_ai else 0,  # AIå¤„ç†é€šå¸¸æœ‰25%çš„è´¨é‡æå‡
                
                # æ–¹æ³•ä¿¡æ¯
                processing_method='ai_intelligent' if use_ai else 'rule_based',
                ai_enabled=use_ai,
                
                # è¯¦ç»†ç»Ÿè®¡
                detailed_stats=result.get('statistics', {}),
            )
            
            # è®°å½•ç›‘æ§æ•°æ®
            if use_ai:
                ai_monitor.record_processing_session(
                    provider=result.get('ai_provider', 'unknown'),
                    tokens_used=result.get('tokens_used', 0),
                    processing_time=ai_processing_time,
                    success=True,
                    quality_score=processing_result.content_quality_score
                )
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ! ç”ŸæˆçŸ¥è¯†åº“æ¡ç›®: {saved_count}ä¸ª")
            return processing_result
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            upload_record.status = 'failed'
            upload_record.error_message = str(e)
            db.session.commit()
            
            return IntelligentProcessingResult(
                success=False,
                upload_id=upload_record.id,
                filename=original_filename,
                original_messages=0,
                raw_conversations=0,
                useful_messages=0,
                noise_filtered=0,
                content_quality_score=0,
                qa_pairs_extracted=0,
                high_quality_pairs=0,
                final_knowledge_entries=0,
                processing_time=time.time() - start_time,
                ai_processing_time=0,
                ai_provider_used='none',
                tokens_consumed=0,
                processing_cost=0,
                extraction_efficiency=0,
                content_improvement_rate=0,
                processing_method='failed',
                ai_enabled=False,
                detailed_stats={},
                error_message=str(e)
            )
    
    async def _read_chat_file(self, file_path: Path) -> Dict[str, Any]:
        """è¯»å–èŠå¤©è®°å½•æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(data)}")
            return data
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            raise
    
    def _preprocess_messages(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é¢„å¤„ç†æ¶ˆæ¯æ•°æ®"""
        messages = []
        
        try:
            # å¤„ç†ä¸åŒæ ¼å¼çš„èŠå¤©è®°å½•
            if isinstance(raw_data, dict):
                if 'messages' in raw_data:
                    messages = raw_data['messages']
                elif 'chats' in raw_data:
                    messages = raw_data['chats']
                elif 'data' in raw_data:
                    messages = raw_data['data']
                else:
                    # å°è¯•ç›´æ¥ä½¿ç”¨æ•°æ®
                    messages = list(raw_data.values()) if isinstance(raw_data, dict) else []
            elif isinstance(raw_data, list):
                messages = raw_data
            
            # æ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼
            processed = []
            for i, msg in enumerate(messages):
                if isinstance(msg, dict):
                    standardized_msg = {
                        'index': i,
                        'timestamp': msg.get('timestamp', msg.get('time', '')),
                        'sender': msg.get('sender', msg.get('from', msg.get('name', f'ç”¨æˆ·{i}'))),
                        'content': msg.get('content', msg.get('message', msg.get('text', ''))),
                        'type': msg.get('type', 'text'),
                        'original': msg
                    }
                    
                    # è¿‡æ»¤ç©ºæ¶ˆæ¯
                    if standardized_msg['content'].strip():
                        processed.append(standardized_msg)
            
            logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(raw_data) if isinstance(raw_data, list) else 'unknown'} -> {len(processed)} æœ‰æ•ˆæ¶ˆæ¯")
            return processed
            
        except Exception as e:
            logger.error(f"æ¶ˆæ¯é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return []
    
    def _should_use_ai_processing(self, messages: List[Dict[str, Any]]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨AIå¤„ç†"""
        if not self.ai_enabled:
            return False
            
        # æ£€æŸ¥æ¶ˆæ¯æ•°é‡
        if len(messages) < 10:
            logger.info("æ¶ˆæ¯æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†")
            return False
        
        if len(messages) > 5000:
            logger.info("æ¶ˆæ¯æ•°é‡è¿‡å¤šï¼Œå»ºè®®åˆ†æ‰¹AIå¤„ç†")
            # å¯ä»¥è€ƒè™‘åˆ†æ‰¹å¤„ç†
            
        # æ£€æŸ¥AIæä¾›å•†å¯ç”¨æ€§
        from .ai_config import ai_config_manager
        primary_provider = ai_config_manager.get_primary_provider()
        if not primary_provider or not ai_config_manager.can_make_request(primary_provider):
            logger.warning("AIæä¾›å•†ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†")
            return False
            
        return True
    
    async def _ai_intelligent_processing(self, 
                                       messages: List[Dict[str, Any]], 
                                       filename: str,
                                       upload_record: UploadHistory) -> Dict[str, Any]:
        """AIæ™ºèƒ½å¤„ç†æµç¨‹"""
        logger.info(f"ğŸ¤– å¼€å§‹AIæ™ºèƒ½å¤„ç†ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # å‡†å¤‡æºä¿¡æ¯
        source_info = {
            'filename': filename,
            'upload_id': upload_record.id,
            'total_messages': len(messages),
            'processing_timestamp': time.time()
        }
        
        # è°ƒç”¨AIå†…å®¹å¤„ç†å™¨
        result = await ai_content_processor.process_chat_content(messages, source_info)
        
        if result['success']:
            logger.info("AIå†…å®¹å¤„ç†å®Œæˆ")
            
            # è·å–AIæä¾›å•†ä¿¡æ¯
            from .ai_config import ai_config_manager
            primary_provider = ai_config_manager.get_primary_provider()
            
            # ä¼°ç®—tokenä½¿ç”¨å’Œæˆæœ¬
            estimated_tokens = len(json.dumps(messages)) // 4  # ç²—ç•¥ä¼°ç®—
            config = ai_config_manager.get_model_config(primary_provider) if primary_provider else None
            estimated_cost = (estimated_tokens / 1000) * (config.cost_per_1k_tokens if config else 0)
            
            result['ai_provider'] = primary_provider or 'unknown'
            result['tokens_used'] = estimated_tokens
            result['processing_cost'] = estimated_cost
            
            return result
        else:
            logger.warning("AIå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™å¤„ç†")
            return await self._rule_based_processing(messages, filename, upload_record)
    
    async def _rule_based_processing(self, 
                                   messages: List[Dict[str, Any]], 
                                   filename: str,
                                   upload_record: UploadHistory) -> Dict[str, Any]:
        """è§„åˆ™å¤„ç†æµç¨‹ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        logger.info(f"ğŸ“‹ å¼€å§‹è§„åˆ™å¤„ç†ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # ä½¿ç”¨ç°æœ‰çš„è§„åˆ™æå–å™¨
        from .data_extractor import DataExtractor
        
        extractor = DataExtractor()
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = []
        for msg in messages:
            formatted_messages.append({
                'timestamp': msg.get('timestamp', ''),
                'sender': msg.get('sender', ''),
                'content': msg.get('content', '')
            })
        
        # æå–é—®ç­”å¯¹
        extracted_pairs = extractor._extract_qa_pairs(formatted_messages, filename)
        
        # è½¬æ¢ä¸ºçŸ¥è¯†åº“æ¡ç›®æ ¼å¼
        knowledge_entries = []
        for pair in extracted_pairs:
            if hasattr(pair, 'question') and hasattr(pair, 'answer'):
                entry = KnowledgeBaseEntry(
                    question=pair.question,
                    answer=pair.answer,
                    category='æœªåˆ†ç±»',
                    confidence=getattr(pair, 'confidence', 0.7),
                    context=getattr(pair, 'context', ''),
                    tags=[],
                    source_info={
                        'filename': filename,
                        'upload_id': upload_record.id,
                        'processing_method': 'rule_based'
                    },
                    quality_score=getattr(pair, 'confidence', 0.7)
                )
                knowledge_entries.append(entry)
        
        return {
            'success': True,
            'knowledge_entries': knowledge_entries,
            'statistics': {
                'original_messages': len(messages),
                'useful_messages': len(messages),  # è§„åˆ™å¤„ç†ä¸åŒºåˆ†
                'noise_filtered': 0,
                'qa_pairs_extracted': len(extracted_pairs),
                'final_knowledge_entries': len(knowledge_entries),
                'content_quality_score': 0.5,  # é»˜è®¤è´¨é‡åˆ†æ•°
                'processing_efficiency': len(knowledge_entries) / len(messages) if messages else 0
            },
            'ai_provider': 'rule_based',
            'tokens_used': 0,
            'processing_cost': 0.0
        }
    
    async def _save_knowledge_entries(self, 
                                    entries: List[KnowledgeBaseEntry], 
                                    upload_record: UploadHistory) -> int:
        """ä¿å­˜çŸ¥è¯†åº“æ¡ç›®åˆ°æ•°æ®åº“"""
        if not entries:
            logger.warning("æ²¡æœ‰çŸ¥è¯†åº“æ¡ç›®éœ€è¦ä¿å­˜")
            return 0
        
        logger.info(f"å¼€å§‹ä¿å­˜{len(entries)}ä¸ªçŸ¥è¯†åº“æ¡ç›®...")
        saved_count = 0
        
        try:
            # è·å–é»˜è®¤åˆ†ç±»
            default_category = Category.query.filter_by(name='æœªåˆ†ç±»').first()
            if not default_category:
                default_category = Category(name='æœªåˆ†ç±»', description='æœªåˆ†ç±»çš„é—®ç­”å†…å®¹')
                db.session.add(default_category)
                db.session.flush()
            
            for entry in entries:
                try:
                    # æŸ¥æ‰¾å¯¹åº”çš„åˆ†ç±»
                    category = Category.query.filter_by(name=entry.category).first()
                    if not category:
                        category = default_category
                    
                    # åˆ›å»ºé—®ç­”å¯¹è®°å½•
                    qa_pair = QAPair(
                        question=entry.question,
                        answer=entry.answer,
                        category_id=category.id,
                        confidence=entry.confidence,
                        upload_id=upload_record.id,
                        context=entry.context,
                        tags=','.join(entry.tags) if entry.tags else '',
                        metadata={
                            'source_info': entry.source_info,
                            'quality_score': entry.quality_score,
                            'ai_processed': entry.source_info.get('processing_method') != 'rule_based'
                        }
                    )
                    
                    db.session.add(qa_pair)
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"ä¿å­˜å•ä¸ªæ¡ç›®å¤±è´¥: {str(e)}")
                    continue
            
            # æäº¤æ‰€æœ‰æ›´æ”¹
            db.session.commit()
            logger.info(f"âœ… æˆåŠŸä¿å­˜{saved_count}ä¸ªçŸ¥è¯†åº“æ¡ç›®")
            
            return saved_count
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥: {str(e)}")
            db.session.rollback()
            return 0


# å…¨å±€å®ä¾‹
intelligent_file_processor = IntelligentFileProcessor()