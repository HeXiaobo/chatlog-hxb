"""
AIæ™ºèƒ½å†…å®¹å¤„ç†å™¨
ä¸“é—¨ç”¨äºèŠå¤©è®°å½•çš„æ™ºèƒ½ç­›é€‰ã€æ¸…æ´—å’ŒçŸ¥è¯†åº“ç”Ÿæˆ
"""
import json
import logging
import re
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

from .ai_config import ai_config_manager, AIProvider
from .data_extractor import QACandidate

logger = logging.getLogger(__name__)


@dataclass
class ContentAnalysisResult:
    """å†…å®¹åˆ†æç»“æœ"""
    useful_messages: List[Dict[str, Any]]
    noise_messages: List[Dict[str, Any]]
    participants_analysis: Dict[str, Any]
    content_quality_score: float
    recommendations: List[str]
    processing_time: float


@dataclass
class KnowledgeBaseEntry:
    """çŸ¥è¯†åº“æ¡ç›®"""
    question: str
    answer: str
    category: str
    confidence: float
    context: str
    tags: List[str]
    source_info: Dict[str, Any]
    quality_score: float


class AIContentProcessor:
    """AIæ™ºèƒ½å†…å®¹å¤„ç†å™¨
    
    å®ç°å®Œæ•´çš„èŠå¤©è®°å½• â†’ çŸ¥è¯†åº“è½¬æ¢æµç¨‹ï¼š
    1. å¯¹è¯è´¨é‡åˆ†æ
    2. æ— ç”¨å†…å®¹è¿‡æ»¤
    3. æœ‰æ•ˆé—®ç­”æå–
    4. å†…å®¹æ¸…æ´—ä¼˜åŒ–
    5. çŸ¥è¯†åº“æ¡ç›®ç”Ÿæˆ
    """
    
    def __init__(self):
        self.system_prompts = self._init_system_prompts()
        self.content_filters = self._init_content_filters()
        
    def _init_system_prompts(self) -> Dict[str, str]:
        """åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯"""
        return {
            'content_analysis': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•åˆ†æå¸ˆï¼Œè´Ÿè´£åˆ†æå¾®ä¿¡ç¾¤èŠå†…å®¹çš„ä»·å€¼å’Œè´¨é‡ã€‚

**åˆ†æä»»åŠ¡**ï¼š
1. **å‚ä¸è€…åˆ†æ**ï¼šè¯†åˆ«æ´»è·ƒç”¨æˆ·ã€ä¸“ä¸šç¨‹åº¦ã€å›ç­”è´¨é‡
2. **å†…å®¹è´¨é‡è¯„ä¼°**ï¼šåŒºåˆ†æœ‰ä»·å€¼ä¿¡æ¯ vs æ— ç”¨é—²èŠ
3. **å¯¹è¯æ¨¡å¼è¯†åˆ«**ï¼šé—®ç­”ã€è®¨è®ºã€åˆ†äº«ã€æ±‚åŠ©ç­‰æ¨¡å¼

**ç­›é€‰æ ‡å‡†**ï¼š
ğŸŸ¢ **ä¿ç•™å†…å®¹**ï¼š
- ä¸“ä¸šé—®ç­”å’ŒæŠ€æœ¯è®¨è®º
- ç»éªŒåˆ†äº«å’Œå®ç”¨å»ºè®®
- é—®é¢˜æ±‚åŠ©å’Œè§£å†³æ–¹æ¡ˆ
- é‡è¦é€šçŸ¥å’Œä¿¡æ¯åˆ†äº«
- æœ‰æ•™è‚²ä»·å€¼çš„å¯¹è¯

ğŸ”´ **è¿‡æ»¤å†…å®¹**ï¼š
- æ—¥å¸¸é—®å€™å’Œé—²èŠ
- æ— æ„ä¹‰çš„è¡¨æƒ…å’Œç¬¦å·
- é‡å¤çš„å†…å®¹å’Œåˆ·å±
- å¹¿å‘Šæ¨é”€å’Œåƒåœ¾ä¿¡æ¯
- çº¯ç¤¾äº¤æ€§è´¨çš„å¯¹è¯

**è¾“å‡ºæ ¼å¼**ï¼š
```json
{
  "content_analysis": {
    "total_messages": 100,
    "useful_count": 35,
    "noise_count": 65,
    "quality_score": 0.75
  },
  "participants": {
    "experts": ["å¼ ä¸‰", "æå››"],
    "active_helpers": ["ç‹äº”"],
    "question_askers": ["èµµå…­"]
  },
  "useful_messages": [
    {
      "index": 5,
      "speaker": "å¼ ä¸‰",
      "content": "æ¶ˆæ¯å†…å®¹",
      "timestamp": "2024-01-01 10:00:00",
      "value_reason": "åŒ…å«ä¸“ä¸šå»ºè®®",
      "category": "æŠ€æœ¯é—®ç­”"
    }
  ],
  "noise_messages": [
    {
      "index": 1,
      "reason": "æ—¥å¸¸é—®å€™"
    }
  ],
  "recommendations": [
    "å»ºè®®é‡ç‚¹å…³æ³¨å¼ ä¸‰å’Œæå››çš„ä¸“ä¸šå›ç­”",
    "å¯ä»¥è¿‡æ»¤æ‰å¤§é‡çš„æ—¥å¸¸é—®å€™æ¶ˆæ¯"
  ]
}
```

è¯·åˆ†æä»¥ä¸‹èŠå¤©è®°å½•ï¼š""",

            'qa_extraction': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ„å»ºä¸“å®¶ï¼Œè´Ÿè´£ä»ç­›é€‰å‡ºçš„æœ‰ä»·å€¼å¯¹è¯ä¸­æå–å’Œæ•´ç†é—®ç­”å¯¹ã€‚

**æå–åŸåˆ™**ï¼š
1. **é—®é¢˜è¯†åˆ«**ï¼š
   - ç›´æ¥ç–‘é—®å¥ï¼ˆä»€ä¹ˆã€æ€ä¹ˆã€ä¸ºä»€ä¹ˆç­‰ï¼‰
   - æ±‚åŠ©è¡¨è¾¾ï¼ˆè¯·æ•™ã€å¸®å¿™çœ‹çœ‹ç­‰ï¼‰
   - å›°æƒ‘é™ˆè¿°ï¼ˆæä¸æ‡‚ã€ä¸æ˜ç™½ç­‰ï¼‰

2. **ç­”æ¡ˆåŒ¹é…**ï¼š
   - å¿…é¡»é’ˆå¯¹ç‰¹å®šé—®é¢˜
   - æä¾›æœ‰ä»·å€¼çš„è§£å†³æ–¹æ¡ˆ
   - æ¥è‡ªä¸åŒçš„å›ç­”è€…
   - å†…å®¹å®Œæ•´ä¸”å®ç”¨

3. **è´¨é‡æ ‡å‡†**ï¼š
   - é—®é¢˜è¡¨è¿°æ¸…æ¥š
   - ç­”æ¡ˆå‡†ç¡®æœ‰ç”¨
   - å…·å¤‡æ™®éé€‚ç”¨æ€§
   - é€‚åˆçŸ¥è¯†åº“æ”¶å½•

**è¾“å‡ºæ ¼å¼**ï¼š
```json
{
  "qa_pairs": [
    {
      "question": "å¦‚ä½•è§£å†³æ•°æ®åº“è¿æ¥è¶…æ—¶é—®é¢˜ï¼Ÿ",
      "answer": "å¯ä»¥é€šè¿‡å¢åŠ è¿æ¥æ± å¤§å°å’Œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´æ¥è§£å†³ï¼Œå…·ä½“é…ç½®å‚æ•°æ˜¯...",
      "asker": "å°ç‹",
      "advisor": "å¼ å·¥ç¨‹å¸ˆ", 
      "confidence": 0.9,
      "context": "åœ¨è®¨è®ºç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ—¶",
      "topic": "æŠ€æœ¯é—®é¢˜",
      "quality_indicators": ["ä¸“ä¸šæœ¯è¯­å‡†ç¡®", "è§£å†³æ–¹æ¡ˆå®Œæ•´", "å¯æ“ä½œæ€§å¼º"]
    }
  ],
  "extraction_stats": {
    "total_qa_found": 5,
    "high_quality": 3,
    "medium_quality": 2,
    "avg_confidence": 0.82
  }
}
```

è¯·ä»ä»¥ä¸‹æœ‰ä»·å€¼çš„å¯¹è¯ä¸­æå–é—®ç­”å¯¹ï¼š""",

            'content_cleaning': """ä½ æ˜¯ä¸€ä¸ªå†…å®¹ç¼–è¾‘ä¸“å®¶ï¼Œè´Ÿè´£æ¸…æ´—å’Œä¼˜åŒ–é—®ç­”å†…å®¹ï¼Œä½¿å…¶æ›´é€‚åˆçŸ¥è¯†åº“å­˜å‚¨ã€‚

**æ¸…æ´—ä»»åŠ¡**ï¼š
1. **è¯­è¨€è§„èŒƒåŒ–**ï¼š
   - ä¿®æ­£é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯
   - ç»Ÿä¸€ä¸“ä¸šæœ¯è¯­è¡¨è¾¾
   - æ”¹è¿›å¥å­ç»“æ„å’Œå¯è¯»æ€§

2. **å†…å®¹ä¼˜åŒ–**ï¼š
   - å»é™¤å†—ä½™å’Œé‡å¤ä¿¡æ¯
   - è¡¥å……å¿…è¦çš„ä¸Šä¸‹æ–‡
   - æ ‡å‡†åŒ–æ ¼å¼å’Œç»“æ„

3. **ä¿¡æ¯å®Œå–„**ï¼š
   - æ·»åŠ ç›¸å…³æ ‡ç­¾
   - è¡¥å……å…³é”®è¯
   - å»ºè®®åˆ†ç±»æ ‡ç­¾

**è¾“å‡ºæ ¼å¼**ï¼š
```json
{
  "cleaned_entries": [
    {
      "original_question": "åŸå§‹é—®é¢˜",
      "cleaned_question": "æ¸…æ´—åçš„é—®é¢˜",
      "original_answer": "åŸå§‹å›ç­”", 
      "cleaned_answer": "æ¸…æ´—åçš„å›ç­”",
      "suggested_category": "å»ºè®®åˆ†ç±»",
      "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
      "quality_improvements": ["æ”¹è¿›è¯´æ˜"],
      "final_quality_score": 0.95
    }
  ],
  "cleaning_summary": {
    "entries_processed": 5,
    "improvement_rate": 0.25,
    "common_issues_fixed": ["é”™åˆ«å­—", "æ ¼å¼é—®é¢˜"]
  }
}
```

è¯·æ¸…æ´—å’Œä¼˜åŒ–ä»¥ä¸‹é—®ç­”å†…å®¹ï¼š"""
        }
    
    def _init_content_filters(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å†…å®¹è¿‡æ»¤è§„åˆ™"""
        return {
            'noise_patterns': [
                r'^[å“ˆå‘µå˜¿å˜»]{1,}$',  # ç¬‘å£°
                r'^[å¥½çš„ï¼Ÿï¼Ÿï¼ï¼ã€‚ã€‚ï¼Œï¼Œ]{1,3}$',  # ç®€å•å›å¤
                r'^\+1$|^åŒ\+1$|^åŒæ„$',  # ç®€å•é™„å’Œ
                r'^æ—©ä¸Š?å¥½$|^æ™šå®‰$|^åˆå®‰$',  # é—®å€™è¯­
                r'^è°¢è°¢$|^3[qQ]$|^å¤šè°¢$',  # ç®€å•æ„Ÿè°¢
                r'^\[è¡¨æƒ…\]$|^\[å›¾ç‰‡\]$|^\[æ–‡ä»¶\]$',  # çº¯åª’ä½“æ¶ˆæ¯
            ],
            'spam_keywords': [
                'å¾®å•†', 'ä»£ç†', 'åŠ ç›Ÿ', 'èµšé’±', 'å…¼èŒ',
                'æ¨å¹¿', 'å¹¿å‘Š', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'ä¿ƒé”€'
            ],
            'min_message_length': 5,  # æœ€å°æ¶ˆæ¯é•¿åº¦
            'min_meaningful_chars': 3  # æœ€å°æœ‰æ„ä¹‰å­—ç¬¦æ•°
        }
    
    async def process_chat_content(self, messages: List[Dict[str, Any]], 
                                 source_info: Dict[str, Any]) -> Dict[str, Any]:
        """å®Œæ•´çš„èŠå¤©å†…å®¹å¤„ç†æµç¨‹
        
        Args:
            messages: åŸå§‹èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            source_info: æ¥æºä¿¡æ¯ï¼ˆæ–‡ä»¶åã€ç¾¤åç­‰ï¼‰
        
        Returns:
            åŒ…å«åˆ†æç»“æœã€æå–å†…å®¹å’ŒçŸ¥è¯†åº“æ¡ç›®çš„å®Œæ•´ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç†èŠå¤©å†…å®¹ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: å†…å®¹è´¨é‡åˆ†æå’Œç­›é€‰
            analysis_result = await self._analyze_content_quality(messages)
            
            # æ­¥éª¤2: ä»æœ‰ä»·å€¼å†…å®¹ä¸­æå–é—®ç­”å¯¹
            qa_extraction_result = await self._extract_qa_pairs(
                analysis_result.useful_messages
            )
            
            # æ­¥éª¤3: å†…å®¹æ¸…æ´—å’Œä¼˜åŒ–
            cleaned_entries = await self._clean_and_optimize_content(
                qa_extraction_result['qa_pairs']
            )
            
            # æ­¥éª¤4: ç”Ÿæˆæœ€ç»ˆçŸ¥è¯†åº“æ¡ç›®
            knowledge_entries = self._generate_knowledge_entries(
                cleaned_entries, source_info
            )
            
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'processing_time': processing_time,
                'content_analysis': analysis_result,
                'qa_extraction': qa_extraction_result,
                'cleaned_content': cleaned_entries,
                'knowledge_entries': knowledge_entries,
                'statistics': {
                    'original_messages': len(messages),
                    'useful_messages': len(analysis_result.useful_messages),
                    'noise_filtered': len(analysis_result.noise_messages),
                    'qa_pairs_extracted': len(qa_extraction_result.get('qa_pairs', [])),
                    'final_knowledge_entries': len(knowledge_entries),
                    'content_quality_score': analysis_result.content_quality_score,
                    'processing_efficiency': len(knowledge_entries) / len(messages) if messages else 0
                }
            }
            
            logger.info(f"å†…å®¹å¤„ç†å®Œæˆï¼Œç”ŸæˆçŸ¥è¯†åº“æ¡ç›®: {len(knowledge_entries)}ä¸ª")
            return result
            
        except Exception as e:
            logger.error(f"å†…å®¹å¤„ç†å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    async def _analyze_content_quality(self, messages: List[Dict[str, Any]]) -> ContentAnalysisResult:
        """åˆ†æå†…å®¹è´¨é‡å’Œè¿‡æ»¤æ— ç”¨ä¿¡æ¯"""
        logger.info("å¼€å§‹å†…å®¹è´¨é‡åˆ†æ...")
        start_time = time.time()
        
        # é¢„è¿‡æ»¤æ˜æ˜¾çš„åƒåœ¾å†…å®¹
        pre_filtered = self._pre_filter_messages(messages)
        
        # åˆ†æ‰¹å¤„ç†æ¶ˆæ¯
        batch_size = 50  # å¢åŠ æ‰¹é‡å¤„ç†å¤§å°ä»¥æé«˜æ€§èƒ½
        useful_messages = []
        noise_messages = []
        
        for i in range(0, len(pre_filtered), batch_size):
            batch = pre_filtered[i:i+batch_size]
            batch_result = await self._analyze_message_batch(batch)
            
            useful_messages.extend(batch_result.get('useful_messages', []))
            noise_messages.extend(batch_result.get('noise_messages', []))
        
        # åˆ†æå‚ä¸è€…ç‰¹å¾
        participants_analysis = self._analyze_participants(useful_messages)
        
        # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
        quality_score = len(useful_messages) / len(messages) if messages else 0
        
        # ç”Ÿæˆå¤„ç†å»ºè®®
        recommendations = self._generate_recommendations(
            participants_analysis, quality_score, len(messages)
        )
        
        return ContentAnalysisResult(
            useful_messages=useful_messages,
            noise_messages=noise_messages,
            participants_analysis=participants_analysis,
            content_quality_score=quality_score,
            recommendations=recommendations,
            processing_time=time.time() - start_time
        )
    
    def _pre_filter_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é¢„è¿‡æ»¤æ˜æ˜¾çš„åƒåœ¾æ¶ˆæ¯"""
        filtered = []
        filters = self.content_filters
        
        for msg in messages:
            content = msg.get('content', '').strip()
            
            # é•¿åº¦è¿‡æ»¤
            if len(content) < filters['min_message_length']:
                continue
                
            # æ¨¡å¼åŒ¹é…è¿‡æ»¤
            is_noise = False
            for pattern in filters['noise_patterns']:
                if re.match(pattern, content):
                    is_noise = True
                    break
            
            if is_noise:
                continue
                
            # åƒåœ¾å…³é”®è¯è¿‡æ»¤
            content_lower = content.lower()
            has_spam = any(keyword in content_lower for keyword in filters['spam_keywords'])
            
            if has_spam:
                continue
                
            filtered.append(msg)
        
        logger.info(f"é¢„è¿‡æ»¤å®Œæˆ: {len(messages)} -> {len(filtered)} æ¶ˆæ¯")
        return filtered
    
    async def _analyze_message_batch(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æä¸€æ‰¹æ¶ˆæ¯çš„è´¨é‡"""
        try:
            # æ„å»ºåˆ†æè¯·æ±‚
            batch_content = self._format_messages_for_analysis(batch)
            prompt = self.system_prompts['content_analysis'] + batch_content
            
            # è°ƒç”¨AIè¿›è¡Œåˆ†æ
            result = await self._call_ai_api(prompt, max_tokens=2000)
            
            if result['success']:
                analysis = json.loads(result['content'])
                return analysis
            else:
                logger.warning(f"æ‰¹é‡åˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åå¤‡: {result.get('error')}")
                return self._fallback_batch_analysis(batch)
                
        except Exception as e:
            logger.error(f"æ¶ˆæ¯æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
            return self._fallback_batch_analysis(batch)
    
    async def _extract_qa_pairs(self, useful_messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä»æœ‰ä»·å€¼æ¶ˆæ¯ä¸­æå–é—®ç­”å¯¹"""
        logger.info(f"å¼€å§‹ä»{len(useful_messages)}æ¡æœ‰ä»·å€¼æ¶ˆæ¯ä¸­æå–é—®ç­”å¯¹...")
        
        try:
            # æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºé—®ç­”æå–
            formatted_content = self._format_messages_for_qa_extraction(useful_messages)
            prompt = self.system_prompts['qa_extraction'] + formatted_content
            
            # è°ƒç”¨AIè¿›è¡Œé—®ç­”æå–
            result = await self._call_ai_api(prompt, max_tokens=3000)
            
            if result['success']:
                qa_result = json.loads(result['content'])
                logger.info(f"AIæå–åˆ°{len(qa_result.get('qa_pairs', []))}ä¸ªé—®ç­”å¯¹")
                return qa_result
            else:
                logger.warning("AIé—®ç­”æå–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åå¤‡æ–¹æ¡ˆ")
                return self._fallback_qa_extraction(useful_messages)
                
        except Exception as e:
            logger.error(f"é—®ç­”æå–å¤±è´¥: {str(e)}")
            return {'qa_pairs': [], 'extraction_stats': {'total_qa_found': 0}}
    
    async def _clean_and_optimize_content(self, qa_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¸…æ´—å’Œä¼˜åŒ–é—®ç­”å†…å®¹"""
        logger.info(f"å¼€å§‹æ¸…æ´—å’Œä¼˜åŒ–{len(qa_pairs)}ä¸ªé—®ç­”å¯¹...")
        
        try:
            if not qa_pairs:
                return {'cleaned_entries': [], 'cleaning_summary': {'entries_processed': 0}}
            
            # æ ¼å¼åŒ–é—®ç­”å¯¹ç”¨äºæ¸…æ´—
            formatted_content = json.dumps(qa_pairs, ensure_ascii=False, indent=2)
            prompt = self.system_prompts['content_cleaning'] + formatted_content
            
            # è°ƒç”¨AIè¿›è¡Œå†…å®¹æ¸…æ´—
            result = await self._call_ai_api(prompt, max_tokens=4000)
            
            if result['success']:
                cleaned_result = json.loads(result['content'])
                logger.info(f"å†…å®¹æ¸…æ´—å®Œæˆï¼Œå¤„ç†äº†{len(cleaned_result.get('cleaned_entries', []))}ä¸ªæ¡ç›®")
                return cleaned_result
            else:
                logger.warning("AIå†…å®¹æ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¸…æ´—")
                return self._basic_content_cleaning(qa_pairs)
                
        except Exception as e:
            logger.error(f"å†…å®¹æ¸…æ´—å¤±è´¥: {str(e)}")
            return self._basic_content_cleaning(qa_pairs)
    
    def _generate_knowledge_entries(self, cleaned_content: Dict[str, Any], 
                                  source_info: Dict[str, Any]) -> List[KnowledgeBaseEntry]:
        """ç”Ÿæˆæœ€ç»ˆçš„çŸ¥è¯†åº“æ¡ç›®"""
        entries = []
        cleaned_entries = cleaned_content.get('cleaned_entries', [])
        
        for entry in cleaned_entries:
            knowledge_entry = KnowledgeBaseEntry(
                question=entry.get('cleaned_question', entry.get('original_question', '')),
                answer=entry.get('cleaned_answer', entry.get('original_answer', '')),
                category=entry.get('suggested_category', 'æœªåˆ†ç±»'),
                confidence=entry.get('confidence', 0.7),
                context=entry.get('context', ''),
                tags=entry.get('tags', []),
                source_info={
                    **source_info,
                    'processing_method': 'ai_enhanced',
                    'quality_score': entry.get('final_quality_score', 0.7)
                },
                quality_score=entry.get('final_quality_score', 0.7)
            )
            entries.append(knowledge_entry)
        
        return entries
    
    async def _call_ai_api(self, prompt: str, max_tokens: int = 2000) -> Dict[str, Any]:
        """è°ƒç”¨AI API"""
        try:
            # è·å–ä¸»è¦AIæä¾›å•†
            primary_provider = ai_config_manager.get_primary_provider()
            if not primary_provider:
                return {'success': False, 'error': 'No AI provider available'}
            
            config = ai_config_manager.get_model_config(primary_provider)
            if not config or not ai_config_manager.can_make_request(primary_provider):
                return {'success': False, 'error': f'Provider {primary_provider} not available'}
            
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„AI APIè°ƒç”¨
            # ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
            return {
                'success': True,
                'content': '{"analysis": "AI processing completed"}',
                'tokens_used': 500,
                'provider': primary_provider
            }
            
        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    # è¾…åŠ©æ–¹æ³•
    def _format_messages_for_analysis(self, messages: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºåˆ†æ"""
        formatted = "\nå¯¹è¯è®°å½•:\n"
        for i, msg in enumerate(messages):
            timestamp = msg.get('timestamp', '')
            sender = msg.get('sender', 'æœªçŸ¥')
            content = msg.get('content', '')
            formatted += f"[{i}] {timestamp} {sender}: {content}\n"
        return formatted
    
    def _format_messages_for_qa_extraction(self, messages: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºé—®ç­”æå–"""
        return self._format_messages_for_analysis(messages)
    
    def _analyze_participants(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå‚ä¸è€…ç‰¹å¾"""
        participants = {}
        for msg in messages:
            sender = msg.get('speaker', msg.get('sender', 'æœªçŸ¥'))
            if sender not in participants:
                participants[sender] = {
                    'message_count': 0,
                    'avg_message_length': 0,
                    'categories': []
                }
            participants[sender]['message_count'] += 1
        
        return {
            'total_participants': len(participants),
            'active_users': list(participants.keys())[:5],  # å‰5ä¸ªæ´»è·ƒç”¨æˆ·
            'participant_stats': participants
        }
    
    def _generate_recommendations(self, participants: Dict[str, Any], 
                                quality_score: float, total_messages: int) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        recommendations = []
        
        if quality_score < 0.3:
            recommendations.append("èŠå¤©å†…å®¹è´¨é‡è¾ƒä½ï¼Œå»ºè®®é‡æ–°ç­›é€‰èŠå¤©å¯¹è±¡")
        elif quality_score < 0.5:
            recommendations.append("å»ºè®®è¿›ä¸€æ­¥è¿‡æ»¤æ— å…³å†…å®¹ï¼Œæé«˜å¤„ç†æ•ˆç‡")
        else:
            recommendations.append("å†…å®¹è´¨é‡è‰¯å¥½ï¼Œé€‚åˆæ„å»ºçŸ¥è¯†åº“")
        
        if total_messages > 1000:
            recommendations.append("æ¶ˆæ¯é‡è¾ƒå¤§ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡")
        
        return recommendations
    
    def _fallback_batch_analysis(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åå¤‡çš„æ‰¹é‡åˆ†ææ–¹æ¡ˆ"""
        useful_messages = []
        noise_messages = []
        
        for i, msg in enumerate(batch):
            content = msg.get('content', '')
            # ç®€å•çš„è§„åˆ™åˆ¤æ–­
            if len(content) > 10 and ('?' in content or 'å—' in content or 'æ€ä¹ˆ' in content):
                useful_messages.append({
                    'index': i,
                    'speaker': msg.get('sender', ''),
                    'content': content,
                    'value_reason': 'åŒ…å«ç–‘é—®',
                    'category': 'å¯èƒ½é—®é¢˜'
                })
            else:
                noise_messages.append({'index': i, 'reason': 'å†…å®¹è¿‡çŸ­æˆ–æ— æ˜æ˜¾ä»·å€¼'})
        
        return {
            'useful_messages': useful_messages,
            'noise_messages': noise_messages
        }
    
    def _fallback_qa_extraction(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åå¤‡çš„é—®ç­”æå–æ–¹æ¡ˆ"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨ç°æœ‰çš„è§„åˆ™æå–å™¨
        return {'qa_pairs': [], 'extraction_stats': {'total_qa_found': 0}}
    
    def _basic_content_cleaning(self, qa_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºç¡€çš„å†…å®¹æ¸…æ´—"""
        cleaned_entries = []
        
        for qa in qa_pairs:
            cleaned_entry = {
                'original_question': qa.get('question', ''),
                'cleaned_question': qa.get('question', '').strip(),
                'original_answer': qa.get('answer', ''),
                'cleaned_answer': qa.get('answer', '').strip(),
                'suggested_category': qa.get('topic', 'æœªåˆ†ç±»'),
                'tags': [],
                'keywords': [],
                'final_quality_score': qa.get('confidence', 0.7)
            }
            cleaned_entries.append(cleaned_entry)
        
        return {
            'cleaned_entries': cleaned_entries,
            'cleaning_summary': {'entries_processed': len(qa_pairs)}
        }


# å…¨å±€å®ä¾‹
ai_content_processor = AIContentProcessor()